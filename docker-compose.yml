# docker compose yaml for airflow sandbox
# celeryexecutor
# postgres meta db 
# redis distributor
# data sources: minio, sql server, extra postgres (simulating snowflake)

version: "3.9" 

# -----------------------
# Airflow Environment
# -----------------------

# defining a yaml anchor so we can reference it from other containers
x-airflow-env: &airflow-env 
  # core settings for airflow
  AIRFLOW_CORE_EXECUTOR: CeleryExecutor
  AIRFLOW_CORE_LOAD_EXAMPLES: "False"     # dont load default airflow exmaples...keep 'er clean
  AIRFLOW_CORE_FERNET_KEY: "${AIRFLOW_FERNET_KEY}"    # MASTER KEY FOR CONNECTIONS (set in .env)
  AIRFLOW_CORE_DAGS_ARE_PAUSED_AT_CREATION: "True"    # NEW dags start paused duh
  AIRFLOW_CORE_DAGS_FOLDER: /opt/airflow/dags         # mounting location for dags
  AIRFLOW_DATABASE_SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow@postgres:5432/airflow  # metadata db 

  # celery -> distributed executor
  AIRFLOW_CELERY_BROKER_URL: redit://redis:6379/0     # celery broker via redis
  AIRFLOW_CELERY_RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow    # celery stores results in the same postgres metadata db

  # webserver and logging 
  AIRFLOW_WEBSERVER_EXPOSE_CONFIG: "True"             # user can see config in web ui 
  AIRFLOW_LOGGING_LOGGING_LEVEL: INFO                 # normal log level

  _PIP_ADDITIONAL_REQUIREMENTS: >
    apache-airflow-providers-amazon
    apache-airflow-providers-microsoft-mssql
    apache-airflow-providers-snowflake

services:
  # ---------------------
  # aiflow metadata db
  # ---------------------

  postgres:
    image: postgres:16
    container_name: af_postgres
    environment:
      POSTGRES_USER: airflow      # db user
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow        # db's name is airflow
    volumes:
      - pg_data:/var/lib/postgresql/data      # PERSIST METADATA ACROSS RESTARTS!
    healthcheck:
      # quick health check for making sure everything boots up in order
      test: ["CMD-SHELL", "pg_isready -u airflow"]
      interval: 10s
      timout: 5s
      retries: 3

  # ---------------
  # Celery broker
  # ---------------

  redis:
    image: redis:7
    container_name: af_redis

  # ----------------------------------------------
  # airflow db init (first time) + admin creation
  # ----------------------------------------------

  airflow-init:
    image: apache/airflow:2.9.3
    container_name: af_init
    depends_on:
      postgres:
        condition: service_healthy    # wait until metadatadb is up 
      redis:
        condition: service_started    # wait until celery distributor is up
      environment:
        <<: *airflow-env
        AIRFLOW_ADMIN_USER: "${AIRFLOW_ADMIN_USER:-admin}"            # pull this stuff from .env or default to admin (-admin does that)
        AIRFLOW_ADMIN_PASSWORD: "${AIRFLOW_ADMIN_PASSWORD:-admin}" 
      entrypoint:
        - bash
        - -lc
        - | 
          set -e
          airflow db upgrade
          airflow users create \
            --role Admin \
            -- username "$AIRFLOW_ADMIN_USER" \
            -- password "$AIRFLOW_ADMIN_PASSWORD" \
            -- firstname Admin --lastname User \
            -- email admin@example.com || true \
          echo "[AIRFLOW] DB READY; ADMIN USER ENSURED"

      volumnes:
        - ./dags:/opt/airflow/dags
        - ./logs:/opt/airflow/logs
        - ./plugins:/opt/airflow/plugins


    # -------------------
    # ariflow webserver
    # -------------------
    
    airflow-webserver:
      image: apache/airflow:2.9.3
      container_name: af_web
      depends_on:
        airflow-init:
          condition: service_completed_successfully     # start when airflow initialized
        environment: *airflow-env
        command: webserver 
        ports: 
          - "8080:8080"
        volumes:
          - ./dags:/opt/airflow/dags        # mount dags from host
          - ./logs:/opt/airflow/logs        # persist logs
          - ./plugins:/opt/airflow/plugins  # direectory to store providers and other plugins

    # -----------------------------------------------------
    # airflow scheduler (parsing dags + scheduling tasks)
    # -----------------------------------------------------
    
    airflow-scheduler:
      image: apache/airflow:2.9.3
      container_name: af_sched
      depends_on:
        airflow-init:
          condition: service_completed_successfully
        environment: *airflow-env         # pull from airflow env above
        command: scheduler
        depends_on:
          aiflow-init:
            condition: service_completed_successfully 
          environment: *airflow-env 
          command: scheduler
        volumes:
            - ./dags:/opt/airflow/dags        # mount dags from host
            - ./logs:/opt/airflow/logs        # persist logs
            - ./plugins:/opt/airflow/plugins  # direectory to store providers and other plugins

    # -----------------------------------------------------
    # airflow triggerer (for deferable operators/sensors) 
    # -----------------------------------------------------

    airflow-triggerer:
      image: apache/airflow:2.9.3
      container_name: af_trigger
      depends_on:
        aiflow-init:
          condition: service_completed_succesfully

    # ------------------------------------------------
    # airflow celery worker (actually executes tasks)
    # ------------------------------------------------

    airflow-worker:
      image: apache/airflow:2.9.3
      container_name: af_worker
      depends_on:
        aiflow-init:
          condition: service_completed_successfully
      environment:
        <<: *airflow-env
        # set global concurrency params 
        AIRFLOW_CORE_PARALLELISM: "8"           # max tasks across cluster
        AIRFLOW_CORE_DAG_CONCURRENCY: "16"      # max active tasks/dag
        AIRFLOW_CELERY_WORKER_CONCURRENCY: "4"  # max processes running per worker container
      command: celery worker
      volumes:
        - ./dags:/opt/airflow/dags        # mount dags from host
        - ./logs:/opt/airflow/logs        # persist logs
        - ./plugins:/opt/airflow/plugins  # direectory to store providers and other plugins
 
    # --------------------------------------------------
    # MinIO (S3- object store for file drops) 
    # --------------------------------------------------
    
    minion:
      image: minio/minio:latest
      container_name: af_minio
      environment:
        MINION_ACCESS_KEY: ${MINIO_ACCESS_KEY:-minio}     # set dev credentials (set in .env)
        MINIIO_SECRET_KEY: ${MINIO_SECRET_KEY:-minio123}
      command: server /data --console-address ":9001"     # set s3 port on :9000; admin console :9001
      ports:
        - "9000:9000"   # s3 api endpoint (used in airflow s3 connections via endpooint_url)
        - "9001:9001"   # minion admin console port
      volumes:
        - minio_data:/data      # persist files 

    # one shot to create a drop bucket; will run after minion launches
    mc:
      image: minion/mc:latest
      container_name: af_minio_mc
      depends_on:
        minio:
          condition: service_started
      entrypoint: >
        sh -c "
        /usr/bin/mc alias set local http://minion:9000 ${MINIO_ACCESS_KEY:-minio} ${MINIO_SECRET_KEY:-minio123} &&
        /usr/bin/mc mb --ignore-existing local/drop &&
        /usr/bin/mc anonymous set public local/drop &&
        echo '[MinIO] drop bucket ready'
        "

    # ----------------------------------------
    # sql server (rip RAM but we gotta do it)
    # ----------------------------------------
   
    mssql:
      image: mcr.microsoft.com/mssql/server:2022-latest
      container_name: af_mssql
      environment:
        ACCEPT_EULA: "Y"  # ofc ms needs this 
        SA_PASSWORD: "${MSSQL_SA_PASSWORD:-mssqlpassword}"  # gotta set a strong password in .env file cuz ms
        MSSQL_PID: "Developer"                              
      ports:
        - "1433:1433"
      volumes:
        - mssql_data:/var/opt/mssql                         # persist db 

      # ------------------
      # additional pg db
      # ------------------

      pg_other:
        image: postgres:16
        container_name: af_pg_other:
        environment:
          POSTGRES_USER: demo
          POSTGRES_PASSWORD: demo
          POSTGRES_DB: demo
        ports:
          - "5433:5432"                                       # avoid clashing with 5433
        volumes:
          - pg_other_data:/var/lib/postgresql/data

      # -------------------------
      # PERSISTENT NAMED VOLUMES
      # -------------------------
      # volumes for containers to mount to?
      volumes:
        pg_data: 
        pg_other_data:
        minio_data:
        mssql_data: 
