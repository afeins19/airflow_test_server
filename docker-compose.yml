version: "3.9"

# -------------------------------------------------------------
# airflow environment (yaml anchor to reuse across services)
# -------------------------------------------------------------

x-airflow-env: &airflow-env
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
  AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  AIRFLOW__CORE__FERNET_KEY: "${AIRFLOW_FERNET_KEY}"
  AIRFLOW__CORE__LOAD_EXAMPLES: "False"
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True"
  AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/dags
  AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
  AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
  AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
  
  # web-wui code editor + file manager
  AIRFLOW__CODE_EDITOR__ENABLED: "True"
  AIRFLOW__CODE_EDITOR__ROOT_DIRECTORY: /opt/airflow/
  AIRFLOW__CODE_EDITOR__GIT_ENABLED: "False" # CHANGE TO TRUE ONCE GIT REPO SETUP FOR DAGS
  AIRFLOW__CODE_EDITOR__MOUNT: "name=logs,path=/opt/airflow/logs"
  AIRFLOW__CODE_EDITOR__MOUNT1: "name=dags,path=/opt/airflow/dags"
  AIRFLOW__CODE_EDITOR__MOUNT2: "name=plugins,path=/opt/airflow/plugins"
  AIRFLOW__CODE_EDITOR__MOUNT3: "name=drop,path=s3://drop"

  AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
  AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minio123}
  AWS_DEFAULT_REGION: us-east-1
  AWS_S3_ADDRESSING_STYLE: path
  AWS_ENDPOINT_URL_S3: http://minio:9000

  # system logging with statsd
  AIRFLOW__METRICS__STATSD: "true"
  AIRFLOW__METRICS__STATSD_HOST: "statsd-exporter"
  AIRFLOW__METRICS__STATSD_PORT: "9125"
  AIRFLOW__METRICS__STATSD_PREFIX: airflow

  # dev convenience: install providers at container start (for a stable setup, bake a custom image)
  # airflow-code-editor -> edit dags in web ui directly
  _pip_additional_requirements: >
    apache-airflow-providers-amazon
    apache-airflow-providers-microsoft-mssql
    apache-airflow-providers-snowflake
    airflow-code-editor==8.1.0
    fs-s3fs
    black
    isort

services:
  # -----------------------
  # airflow metadata db (postgres)
  # -----------------------
  postgres:
    image: postgres:16
    container_name: af_postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # -----------------------
  # celery broker (redis)
  # -----------------------
  redis:
    image: redis:7
    container_name: af_redis

  # -----------------------------------------------
  # one-time airflow db init + admin user creation
  # -----------------------------------------------
  airflow-init:
    image: apache/airflow:2.10.0    # latest version
    container_name: af_init
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    environment:
      <<: *airflow-env
      AIRFLOW_ADMIN_USER: "${AIRFLOW_ADMIN_USER:-admin}"
      AIRFLOW_ADMIN_PASSWORD: "${AIRFLOW_ADMIN_PASSWORD:-admin}"
      
      # setting airflow perms to match what we set on thisstem 
      AIRFLOW_UID: "50000"
    entrypoint:
      - bash
      - -lc
      - |
        set -e
        airflow db upgrade
        airflow users create \
          --role Admin \
          --username "$AIRFLOW_ADMIN_USER" \
          --password "$AIRFLOW_ADMIN_PASSWORD" \
          --firstname admin --lastname user \
          --email admin@example.com || true
        echo "[airflow] db ready; admin user ensured"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins

  # -------------------
  # airflow webserver
  # -------------------
  airflow-webserver:
    image: apache/airflow:2.10.0
    container_name: af_web
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file: [./.env]         # inject from anchor
    environment: *airflow-env
    command: ["airflow", "webserver", "--port", "8080"] # explicit startup command for module
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins

  # -----------------------------------------------------
  # airflow scheduler (parses dags & schedules tasks)
  # -----------------------------------------------------
  airflow-scheduler:
    image: apache/airflow:2.10.0
    container_name: af_sched
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file: [./.env]         # inject from anchor
    environment: *airflow-env
    command: ["airflow", "scheduler"]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins

  # -----------------------------------------------------
  # airflow triggerer (for deferrable operators/sensors)
  # -----------------------------------------------------
  airflow-triggerer:
    image: apache/airflow:2.10.0
    container_name: af_trigger
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file: [./.env]         # inject from anchor 
    environment: *airflow-env
    command: ["airflow", "triggerer"]
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins

  # ------------------------------------------------
  # airflow celery worker (executes tasks)
  # ------------------------------------------------
  airflow-worker:
    image: apache/airflow:2.10.0
    container_name: af_worker
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file: [./.env]         # inject from anchor
    environment:
      <<: *airflow-env
      # global concurrency knobs (tune for your box/workload)
      airflow__core__parallelism: "8"           # max tasks across the cluster
      airflow__core__dag_concurrency: "16"      # max running tasks per dag
      airflow__celery__worker_concurrency: "4"  # processes per worker container
    command: celery worker
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins

  # --------------------------------------------------
  # minio (s3-compatible object store for "file drops")
  # --------------------------------------------------
  minio:
    image: minio/minio:latest
    container_name: af_minio
    env_file: [./.env]
    environment:
      MINIO_ROOT_USER: "${MINIO_ROOT_USER:-minio}"
      MINIO_ROOT_PASSWORD: "${MINIO_ROOT_PASSWORD:-minio123}"
    command: server --console-address ":9001" /data # run in server mode 
    ports:
      - "9000:9000"   # s3 api
      - "9001:9001"   # minio web console
    volumes:
      - minio_data:/data

  # one-shot helper to create a 'drop' bucket after minio starts
  mc:
    image: minio/mc:latest
    container_name: af_minio_mc
    depends_on:
      minio:
        condition: service_started
    entrypoint: >
      sh -c "
      /usr/bin/mc alias set local http://minio:9000 ${MINIO_ROOT_USER:-minio} ${MINIO_ROOT_PASSWORD:-minio123} &&
      /usr/bin/mc mb --ignore-existing local/drop &&
      /usr/bin/mc anonymous set public local/drop &&
      echo '[minio] drop bucket ready'
      "

  # ----------------------------------------
  # microsoft sql server (developer)
  # ----------------------------------------
  mssql:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: af_mssql
    environment:
      ACCEPT_EULA: "y"
      sa_password: "${MSSQL_SA_PASSWORD:-bababababad!}"  # set in .env
      mssql_pid: "developer"
    ports:
      - "1433:1433"
    volumes:
      - mssql_data:/var/opt/mssql

  # ------------------
  # extra postgres db
  # ------------------
  # simulating an additional data source like snowflake (using postgres) 
  pg_imdb:
    image: postgres:16
    container_name: af_pg_imdb
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: demo
      postgres_db: demo
    ports:
      - "5433:5432"     # avoid clashing with metadata postgres
    volumes:
      - pg_other_data:/var/lib/postgresql/data


  # ----------------------
  # system monitoring
  # ----------------------

  # -----------------------
  # statsd -> prometheus bridge for airflow metrics
  # -----------------------
  statsd-exporter:
    image: prom/statsd-exporter:latest
    container_name: mon_statsd_exporter
    command: --log.level=info
    # listens on 9125/udp for statsd, exposes prometheus metrics on 9102
    ports:
      - "9102:9102"
      - "9125:9125/udp"

  # -----------------------
  # cadvisor (container cpu/ram/io)
  # -----------------------
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: mon_cadvisor
    # port 8080 inside; map to 8082 on host so it doesn't clash with airflow
    ports:
      - "8082:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
    privileged: true

  # -----------------------
  # postgres exporters (airflow meta + extra pg)
  # -----------------------
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: mon_pg_exporter_meta
    environment:
      # airflow metadata db
      DATA_SOURCE_NAME: postgresql://airflow:airflow@postgres:5432/airflow?sslmode=disable
    # prom metrics on 9187
    expose: ["9187"]
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", " wget -qO- http://localhost:9187/metrics | head -n1 || exit 1"]
      interval: 10s
      timeout: 4s
      retries: 6

  pg-imdb-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: mon_pg_exporter_imbdb
    environment:
      # extra postgres log for imdb simulation
      DATA_SOURCE_NAME: postgresql://demo:demo@af_pg_other:5432/demo?sslmode=disable
    expose: ["9187"]
    depends_on:
      pg_imdb:
        condition: service_started

    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9187/metrics | head -n1 || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 6

  # -----------------------
  # redis exporter (celery broker)
  # -----------------------
  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: mon_redis_exporter
    environment:
      redis_addr: redis://redis:6379
    expose: ["9121"]

  # -----------------------
  # prometheus (scrapes all exporters)
  # -----------------------
  prometheus:
    image: prom/prometheus:latest
    container_name: mon_prometheus
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --storage.tsdb.retention.time=15d
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    depends_on:
      - statsd-exporter
      - cadvisor
      - postgres-exporter
      - pg-imdb-exporter
      - redis-exporter

  # -----------------------
  # grafana (dashboards)
  # -----------------------
  grafana:
    image: grafana/grafana:latest
    container_name: mon_grafana
    environment:
      gf_security_admin_password: "${grafana_admin_password:-admin}"
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus

# -------------------------
# persistent named volumes
# -------------------------
volumes:
  pg_data:
  pg_other_data:
  minio_data:
  mssql_data:
  prometheus_data:
  grafana_data:

